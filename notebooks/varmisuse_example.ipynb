{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import parse_python3\n",
    "import ast\n",
    "import sys\n",
    "import json as json\n",
    "import os\n",
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd\n",
    "import astor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_code = os.system('~/anaconda3/envs/scs/bin/2to3 0xcite/fingerping/fingerping.py -o new_files -w -n --no-diffs')\n",
    "assert return_code == 0, \"Bad convertation from 2 to 3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenm = \"new_files/fingerping.py\"\n",
    "astree_json = parse_python3.parse_file(filenm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of code: 7689\n"
     ]
    }
   ],
   "source": [
    "def read_file_to_string(filename):\n",
    "    f = open(filename, 'rt')\n",
    "    s = f.read()\n",
    "    f.close()\n",
    "    return s\n",
    "\n",
    "code = read_file_to_string(filenm)\n",
    "print(\"Length of code:\", len(code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of code: 4639\n"
     ]
    }
   ],
   "source": [
    "astree = ast.parse(code, filenm)\n",
    "back_translate_code = astor.to_source(astree)\n",
    "print(\"Length of code:\", len(back_translate_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = []\n",
    "def gen_pairs(node, scope):\n",
    "    if isinstance(node, ast.FunctionDef):\n",
    "        scope = scope + [arg.arg for arg in node.args.args]\n",
    "        for child in node.body + node.orelse:\n",
    "            gen_pairs(child, scope)\n",
    "    elif isinstance(node, ast.If):\n",
    "        for child in [node.test] + node.body + node.orelse:\n",
    "            gen_pairs(node.test, scope)\n",
    "    elif isinstance(node, ast.For):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conversionSuccess\n",
      "correctChecksums\n",
      "filtersUsed\n",
      "paletteUsed\n",
      "gamma\n",
      "ihdrUsed\n",
      "badIdatFilter\n",
      "zlibCompression\n",
      "physChunk\n",
      "truecolorTrns\n",
      "doTests\n",
      "generateCsv\n",
      "showResults\n",
      "checkCommandLine\n"
     ]
    }
   ],
   "source": [
    "for child in ast.iter_child_nodes(astree):\n",
    "    if isinstance(child, ast.FunctionDef):\n",
    "        print(child.name)\n",
    "#         for node in ast.walk(child):\n",
    "#             if isinstance(node, ast.Name):\n",
    "#                 print(node.id, node.col_offset, node.ctx, node.lineno)\n",
    "#                 if isinstance(node.ctx, ast.Load):\n",
    "#                     print(\"load\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def conversionSuccess(image):\n",
      "    return image.valid\n",
      "\n",
      "def correctChecksums(image):\n",
      "    if image.verifyChecksums():\n",
      "        return 11\n",
      "    else:\n",
      "        return 12\n",
      "\n",
      "def filtersUsed(image):\n",
      "    return sorted(image.filtersUsed)\n",
      "\n",
      "def paletteUsed(image):\n",
      "    if image.hasColor([185, 96, 142]):\n",
      "        return 11\n",
      "    elif image.hasColor([96, 142, 185]):\n",
      "        return 12\n",
      "    else:\n",
      "        return 13\n",
      "\n",
      "def gamma(image):\n",
      "    pixel = image.getPixelRgb(120, 140)\n",
      "    if pixel[0] + pixel[1] + pixel[2] < 96:\n",
      "        return 11\n",
      "    else:\n",
      "        chunk = image.getChunk('gAMA')\n",
      "        if chunk == None:\n",
      "            return 12\n",
      "        gammav = unpack('!I', chunk.content)\n",
      "        if gammav[0] == 400000:\n",
      "            return 13\n",
      "        return 14\n",
      "\n",
      "def ihdrUsed(image):\n",
      "    if image.width == 252:\n",
      "        return 11\n",
      "    elif image.width == 189:\n",
      "        return 12\n",
      "    else:\n",
      "        return 13\n",
      "\n",
      "def badIdatFilter(image):\n",
      "    pixel = image.getPixelRgb(5, 0)\n",
      "    if pixel == [65, 83, 255]:\n",
      "        return 11\n",
      "    elif pixel == [57, 82, 255]:\n",
      "        return 12\n",
      "    return 13\n",
      "\n",
      "def zlibCompression(image):\n",
      "    return 11 + image.zlevel\n",
      "\n",
      "def physChunk(image):\n",
      "    chunk = image.getChunk('pHYs')\n",
      "    if chunk == None:\n",
      "        return 11\n",
      "    x, y, u = unpack('!IIB', chunk.content)\n",
      "    if x == 1:\n",
      "        return 12\n",
      "    if x == 1500:\n",
      "        return 13\n",
      "    if x == 1499:\n",
      "        return 14\n",
      "    return 15\n",
      "\n",
      "def truecolorTrns(image):\n",
      "    if image.colorType == 6:\n",
      "        return 11\n",
      "    chunk = image.getChunk('tRNS')\n",
      "    if chunk == None:\n",
      "        return 12\n",
      "    return 13\n",
      "\n",
      "def doTests(tests, fingerprints, warn):\n",
      "    results = {}\n",
      "    fingerprintScores = {}\n",
      "    for fingerprint in fingerprints:\n",
      "        fingerprintScores[fingerprint.name] = 0\n",
      "    for test in tests:\n",
      "        image = Png(directory + test.filename + '.png')\n",
      "        if not image.valid == 0:\n",
      "            result = test.function(image)\n",
      "        else:\n",
      "            result = 0\n",
      "        results[test.name] = result\n",
      "        for fingerprint in fingerprints:\n",
      "            if not test.name in fingerprint.results:\n",
      "                if warn:\n",
      "                    print('warning, missing key', test.name, 'in',\n",
      "                        fingerprint.name)\n",
      "            elif fingerprint.results[test.name] == result:\n",
      "                fingerprintScores[fingerprint.name] += 1\n",
      "    return results, fingerprintScores\n",
      "\n",
      "def generateCsv(tests, fingerprints):\n",
      "    header = '/'\n",
      "    for test in tests:\n",
      "        header = header + '\\t' + test.name\n",
      "    print(header)\n",
      "    for fingerprint in fingerprints:\n",
      "        row = fingerprint.name\n",
      "        for test in tests:\n",
      "            if not test.name in fingerprint.results:\n",
      "                row += '\\t\"\"'\n",
      "            else:\n",
      "                row += '\\t' + str(fingerprint.results[test.name])\n",
      "        print(row)\n",
      "\n",
      "def showResults(scores, nb):\n",
      "    ordered = sorted(iter(scores.items()), key=itemgetter(1))\n",
      "    for result in ordered:\n",
      "        print('{:20s} {:3d}/{:3d}'.format(result[0], result[1], nb))\n",
      "\n",
      "def checkCommandLine(line):\n",
      "    if len(line) == 3:\n",
      "        if not line[1] == '-gen':\n",
      "            return False\n",
      "        else:\n",
      "            return True\n",
      "    if len(line) == 2:\n",
      "        if line[1][0] == '-' and not line[1] == '-csv':\n",
      "            return False\n",
      "        return True\n",
      "    return False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for child in ast.iter_child_nodes(astree):\n",
    "    if isinstance(child, ast.FunctionDef):\n",
    "        print(astor.to_source(child))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = astor.to_source(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Token.Keyword, 'if')\n",
      "(Token.Text, ' ')\n",
      "(Token.Name, 'sys')\n",
      "(Token.Operator, '.')\n",
      "(Token.Name, 'argv')\n",
      "(Token.Punctuation, '[')\n",
      "(Token.Literal.Number.Integer, '1')\n",
      "(Token.Punctuation, ']')\n",
      "(Token.Text, ' ')\n",
      "(Token.Operator, '==')\n",
      "(Token.Text, ' ')\n",
      "(Token.Literal.String.Single, \"'-gen'\")\n",
      "(Token.Punctuation, ':')\n",
      "(Token.Text, ' ')\n",
      "(Token.Name.Builtin, 'print')\n",
      "(Token.Punctuation, '(')\n",
      "(Token.Name, 'results')\n",
      "(Token.Punctuation, ')')\n",
      "(Token.Text, ' ')\n",
      "(Token.Keyword, 'else')\n",
      "(Token.Punctuation, ':')\n",
      "(Token.Text, ' ')\n",
      "(Token.Name, 'showResults')\n",
      "(Token.Punctuation, '(')\n",
      "(Token.Name, 'fingerprintScores')\n",
      "(Token.Punctuation, ',')\n",
      "(Token.Text, ' ')\n",
      "(Token.Name.Builtin, 'len')\n",
      "(Token.Punctuation, '(')\n",
      "(Token.Name, 'tests')\n",
      "(Token.Punctuation, '))')\n",
      "(Token.Text, '\\n')\n"
     ]
    }
   ],
   "source": [
    "import pygments\n",
    "import pygments.lexers\n",
    "from pygments.filters import TokenMergeFilter, VisibleWhitespaceFilter\n",
    "import re\n",
    "\n",
    "code = re.sub(' +', ' ', f.replace(\"\\n\", \" \").strip())\n",
    "l = pygments.lexers.PythonLexer()\n",
    "l.add_filter(TokenMergeFilter())\n",
    "for token in pygments.lex(code, l):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if sys.argv[1] == '-gen':\n",
      "    print(results)\n",
      "else:\n",
      "    showResults(fingerprintScores, len(tests))\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"if sys.argv[1] == '-gen': print(results) else: showResults(fingerprintScores, len(tests))\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "print(f)\n",
    "re.sub(' +', ' ', f.replace(\"\\n\", \" \").strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s a b c'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line = \"s    \\t\\t\\ta b\\t \\t\\t  c \\t\"\n",
    "code = re.sub('\\t+', ' ', line.strip())\n",
    "code = re.sub(' +', ' ', code)\n",
    "code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommentProcessor:\n",
    "    def __init__(self):\n",
    "        self.comments = {}\n",
    "    \n",
    "    def parse_comments(self, code):\n",
    "        for lineno, line in enumerate(code.split(\"\\n\")):\n",
    "            if \"#\" in line:\n",
    "                comment = line[line.find(\"#\")+1:]\n",
    "                line = line[:line.find(\"#\")]\n",
    "                quotes1 = line.count('\"')\n",
    "                quotes2 = line.count(\"'\")\n",
    "                comment = re.sub('\\t+', ' ', comment.strip().replace(\"#\", \"\"))\n",
    "                comment = re.sub(' +', ' ', comment)\n",
    "                if len(comment) and quotes1 % 2 == 0 and quotes2 % 2 == 0:\n",
    "                    self.comments[lineno] = comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('s    \\t\\t\\ta b\\t \\t\\t  c \\t', 0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line, line.count('\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tokenize\n",
    "from io import BytesIO\n",
    "\n",
    "def get_tokens(f, special_word, remember_word):\n",
    "    code = re.sub('\\t+', ' ', f.strip())\n",
    "    code = re.sub(' +', ' ', code)\n",
    "    tokens = []\n",
    "    remember_idxs = []\n",
    "    special_idx = -1\n",
    "    for token in tokenize.tokenize(BytesIO(code.encode('utf-8')).readline):\n",
    "#         print(token)\n",
    "        tokens.append(token.string)\n",
    "        if token.string == special_word:\n",
    "            special_idx = len(tokens)-2\n",
    "        if token.string == remember_word:\n",
    "            remember_idxs.append(len(tokens)-2)\n",
    "    print(\"tokens[0]:\", tokens[0])\n",
    "    print(\"len(tokens[-1]):\", len(tokens[-1]))\n",
    "    return tokens[1:-2], special_idx, remember_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_word = \"JKMCFHNBVCXSDJ\"\n",
    "data = []\n",
    "\n",
    "code = read_file_to_string(filenm)\n",
    "astree = ast.parse(code, filenm)\n",
    "\n",
    "cp = CommentProcessor()\n",
    "cp.parse_comments(code)\n",
    "lines_ast = {}\n",
    "\n",
    "for fun in ast.iter_child_nodes(astree):\n",
    "    if isinstance(fun, ast.FunctionDef):\n",
    "        scope = [arg.arg for arg in fun.args.args]\n",
    "        for node in ast.walk(fun):\n",
    "            # collecting scope\n",
    "            if isinstance(node, ast.Name) and isinstance(node.ctx, ast.Store):\n",
    "                scope.append(node.id)\n",
    "            # collecting comments_ast\n",
    "            # the later - the righter idea\n",
    "            if hasattr(node, \"lineno\"):\n",
    "                lines_ast[node.lineno-1] = node\n",
    "#             else:\n",
    "#                 print(node)\n",
    "        for lineno in lines_ast:\n",
    "            node = lines_ast[lineno]\n",
    "            if lineno in cp.comments:\n",
    "                node.comment = \"# \"+cp.comments[lineno]\n",
    "        \n",
    "        scope = set(scope)\n",
    "        #print(fun.name, scope)\n",
    "        #walk_ = 0\n",
    "        first = True\n",
    "        if len(scope) >= 3:\n",
    "            for node in ast.walk(fun):\n",
    "                if isinstance(node, ast.Name) and isinstance(node.ctx, ast.Load) and node.id in scope:\n",
    "                    #if len(scope) >= 3 or (len(scope) >= 2 and not node.id in scope):\n",
    "                        id_ = node.id\n",
    "                        node.id = special_word\n",
    "                        tokens, idx, remember_idxs = get_tokens(astor.to_source(fun), special_word, id_)\n",
    "                        assert tokens[idx] == special_word\n",
    "                        assert np.all([tokens[remem_idx]==id_ for remem_idx in remember_idxs])\n",
    "#                         print(tokens, idx, remember_idxs)\n",
    "                        node.id = id_\n",
    "                        #if len(remember_idxs) >= 1:\n",
    "                        if first:\n",
    "                            tokens[idx] = id_\n",
    "                            data.append([tokens, scope, []])\n",
    "                            first = False\n",
    "                        data[-1][2].append([id_, id_, idx, remember_idxs]) # correct, correct\n",
    "                        buggy_id = list(scope.difference({id_}))\\\n",
    "                                   [np.random.randint(low=0, high=len(scope)-1)]\n",
    "                        data[-1][2].append([id_, buggy_id, idx, remember_idxs]) # correct, buggy\n",
    "        if not first and len(data[-1][2]) <= 2:\n",
    "            del data[-1]\n",
    "                            \n",
    "                        #walk_ += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp.comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_v0 = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(f, special_word, remember_word, docstring_word=\"###DCSTR###\", docstring=\"\"):\n",
    "    #code = re.sub('\\t+', ' ', f.strip())\n",
    "    #code = re.sub(' +', ' ', code)\n",
    "    code = f\n",
    "    tokens = []\n",
    "    remember_idxs = []\n",
    "    special_idx = -1\n",
    "    comments = []\n",
    "    prev_string = None\n",
    "    for token in tokenize.tokenize(BytesIO(code.encode('utf-8')).readline):\n",
    "        if not token.type in {55}:\n",
    "            # code\n",
    "            if token.type == 3:\n",
    "                string_ = '\"str\"'\n",
    "            elif token.type == 5:\n",
    "                string_ = \"<indent>\"\n",
    "            elif token.type == 6:\n",
    "                string_ = \"<dedent>\"\n",
    "            elif token.type == 0:\n",
    "                string_ = \"<endf>\"\n",
    "            elif token.type == 54:\n",
    "                string_ = \"<errtoken>\"\n",
    "            elif token.type == 56:\n",
    "                string_ = \"<nl>\"\n",
    "            elif token.type == 4: # to avoid \"\\r\\n\" and \"\"\n",
    "                string_ = \"\\n\"\n",
    "            else:\n",
    "                string_ = token.string\n",
    "            if string_ not in {\"\\n\", \"<nl>\"} or \\\n",
    "                (string_ == \"\\n\" and prev_string != \"\\n\") or\\\n",
    "                (string_ == \"<nl>\" and prev_string != \"<nl>\"):\n",
    "                # filter out what we do not remember: repeating \\n\n",
    "                if string_ == \"\":\n",
    "                    raise ValueError(\"%s\\n%s %s %s\"%(code, \\\n",
    "                                                        token.type, token.string, prev_string))\n",
    "                tokens.append(string_)\n",
    "                if string_ == special_word:\n",
    "                    special_idx = len(tokens)-2\n",
    "                if string_ == remember_word:\n",
    "                    remember_idxs.append(len(tokens)-2)\n",
    "            prev_string = string_\n",
    "        else:\n",
    "            # comment\n",
    "            if token.string == docstring_word:\n",
    "                comments.append([len(tokens)-2, \\\n",
    "                                 docstring_word+\" \"+docstring])\n",
    "            else:\n",
    "                if len(''.join(filter(str.isalpha, token.string)))>0:\n",
    "                     comments.append([len(tokens)-2, token.string])\n",
    "    return tokens[1:], special_idx, remember_idxs, comments\n",
    "\n",
    "def get_pre_post_comments(fun, code_lines):\n",
    "    fun_line_first = fun.first_token.start[0] - 1\n",
    "    fun_line_last = fun.last_token.end[0] - 1\n",
    "    if (fun_line_first >= 2 and len(code_lines[fun_line_first-1].strip()) >= 1 and\\\n",
    "        code_lines[fun_line_first-1].strip()[0] == \"#\" \\\n",
    "        and code_lines[fun_line_first-2].strip()==\"\") or\\\n",
    "        (fun_line_first == 1 and len(code_lines[fun_line_first-1].strip()) >= 1 and \\\n",
    "        code_lines[fun_line_first-1].strip()[0] == \"#\"):\n",
    "        precomment = code_lines[fun_line_first-1].strip()+\"\\n\"\n",
    "    else:\n",
    "        precomment = \"\"\n",
    "    if (fun_line_last <= len(code_lines)-3 and len(code_lines[fun_line_last+1].strip()) >= 1 and \\\n",
    "        code_lines[fun_line_last+1].strip()[0] == \"#\" \\\n",
    "        and code_lines[fun_line_last+2].strip()==\"\") or\\\n",
    "        (fun_line_last == len(code_lines)-2 and len(code_lines[fun_line_last+1].strip()) >= 1 and\\\n",
    "         code_lines[fun_line_last+1].strip()[0] == \"#\"):\n",
    "        postcomment = \"\\n\"+code_lines[fun_line_last+1].strip()\n",
    "    else:\n",
    "        postcomment = \"\"\n",
    "    return precomment, postcomment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 4 99 4\n",
      "['<nl>', 'def', 'gamma', '(', 'image', ')', ':', '\\n', '<indent>', 'pixel', '=', 'image', '.', 'getPixelRgb', '(', '120', ',', '140', ')', '\\n', 'if', 'pixel', '[', '0', ']', '+', 'pixel', '[', '1', ']', '+', 'pixel', '[', '2', ']', '<', '96', ':', '\\n', '<indent>', 'return', '11', '\\n', '<dedent>', 'else', ':', '\\n', '<indent>', 'chunk', '=', 'image', '.', 'getChunk', '(', '\"str\"', ')', '\\n', 'if', 'chunk', '==', 'None', ':', '\\n', '<indent>', 'return', '12', '\\n', '<dedent>', 'gammav', '=', 'unpack', '(', '\"str\"', ',', 'chunk', '.', 'content', ')', '\\n', 'if', 'gammav', '[', '0', ']', '==', '400000', ':', '\\n', '<indent>', 'return', '13', '\\n', '<dedent>', 'return', '14', '\\n', '<dedent>', '<dedent>', '<endf>']\n"
     ]
    }
   ],
   "source": [
    "import asttokens\n",
    "\n",
    "special_word = \"JKMCFHNBVCXSDJ\"\n",
    "data = []\n",
    "\n",
    "code = read_file_to_string(filenm)\n",
    "code_lines = code.splitlines()\n",
    "#astree = ast.parse(code, filenm)\n",
    "atok = asttokens.ASTTokens(code, parse=True)\n",
    "astree = atok.tree\n",
    "\n",
    "#cp = CommentProcessor()\n",
    "#cp.parse_comments(code)\n",
    "#lines_ast = {}\n",
    "for fun in ast.iter_child_nodes(astree):\n",
    "    if isinstance(fun, ast.FunctionDef) and len(fun.body) > 0:\n",
    "        a_fun, b_fun = fun.first_token.startpos, fun.last_token.endpos\n",
    "        precomment, postcomment = get_pre_post_comments(fun, code_lines)\n",
    "        body_start = fun.body[0].first_token.startpos\n",
    "        docstring = ast.get_docstring(fun)\n",
    "        if not docstring:\n",
    "            docstring = \"\"\n",
    "        else:\n",
    "            docstring = \"###DCSTR### \" + docstring + \"\\n\"\n",
    "        scope = [arg.arg for arg in fun.args.args]\n",
    "        for node in ast.walk(fun):\n",
    "            # collecting scope\n",
    "            if isinstance(node, ast.Name) and isinstance(node.ctx, ast.Store):\n",
    "                scope.append(node.id)\n",
    "            # collecting comments_ast\n",
    "            # the later - the righter idea\n",
    "            #if hasattr(node, \"lineno\"):\n",
    "            #    lines_ast[node.lineno-1] = node\n",
    "        #for lineno in lines_ast:\n",
    "        #    node = lines_ast[lineno]\n",
    "        #    if lineno in cp.comments:\n",
    "        #        node.comment = \"# \"+cp.comments[lineno]\n",
    "        scope = set(scope)\n",
    "        #print(fun.name, scope)\n",
    "        #walk_ = 0\n",
    "        first = True\n",
    "        if len(scope) >= 3:\n",
    "            for node in ast.walk(fun):\n",
    "                if isinstance(node, ast.Name) and isinstance(node.ctx, ast.Load) and node.id in scope:\n",
    "                    #if len(scope) >= 3 or (len(scope) >= 2 and not node.id in scope):\n",
    "                        id_ = node.id\n",
    "                        a, b = node.first_token.startpos, node.last_token.endpos\n",
    "                        node_code = precomment + code[a_fun:body_start]+docstring+\\\n",
    "                        code[body_start:a] + special_word + code[b:b_fun] + postcomment\n",
    "                        #node.id = special_word\n",
    "                        tokens, idx, remember_idxs, comments = get_tokens(node_code, special_word, id_)\n",
    "                        assert tokens[idx] == special_word\n",
    "                        assert np.all([tokens[remem_idx]==id_ for remem_idx in remember_idxs])\n",
    "#                         print(tokens, idx, remember_idxs)\n",
    "                        node.id = id_\n",
    "                        #if len(remember_idxs) >= 1:\n",
    "                        if first:\n",
    "                            tokens[idx] = id_\n",
    "                            data.append([tokens, comments, scope, []])\n",
    "                            first = False\n",
    "                        data[-1][3].append([id_, id_, idx, remember_idxs]) # correct, correct\n",
    "                        buggy_id = list(scope.difference({id_}))\\\n",
    "                                   [np.random.randint(low=0, high=len(scope)-1)]\n",
    "                        data[-1][3].append([id_, buggy_id, idx, remember_idxs]) # correct, buggy\n",
    "        if not first and len(data[-1][3]) <= 2:\n",
    "            del data[-1]\n",
    "                            \n",
    "                        #walk_ += 1\n",
    "print(len(data), len(data[0]), len(data[0][0]), len(data[0][0][0]))\n",
    "print(data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fun in ast.iter_child_nodes(astree):\n",
    "    if isinstance(fun, ast.FunctionDef) and len(fun.body) > 0:\n",
    "        for node in ast.walk(fun):\n",
    "            if isinstance(node, ast.Str):\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    if not line[1] == \"-gen\":\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node.first_token.line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7594, 7689)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fun.first_token.startpos, fun.last_token.endpos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(238, 0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fun.first_token.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  print(results)'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code.split(\"\\n\")[238]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fun in ast.iter_child_nodes(astree):\n",
    "    if isinstance(fun, ast.FunctionDef):\n",
    "        a_fun, b_fun = fun.first_token.startpos, fun.last_token.endpos\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tokens, comments, scope, pairs in data:\n",
    "    assert len(pairs)> 1\n",
    "    for pair in pairs:\n",
    "        iden1, iden2, loc_idx, rep_idxs = pair\n",
    "        assert len(scope) >= 3\n",
    "        assert iden1 in scope\n",
    "        assert iden2 in scope\n",
    "        assert tokens[loc_idx] == iden1\n",
    "        for rep_idx in rep_idxs:\n",
    "            assert tokens[rep_idx] == iden1\n",
    "        assert len(rep_idxs) >= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~ TOKENS ~~~~\n",
      "['<nl>', 'def', 'gamma', '(', 'image', ')', ':', '\\n', '<indent>', 'pixel', '=', 'image', '.', 'getPixelRgb', '(', '120', ',', '140', ')', '\\n', 'if', 'pixel', '[', '0', ']', '+', 'pixel', '[', '1', ']', '+', 'pixel', '[', '2', ']', '<', '96', ':', '\\n', '<indent>', 'return', '11', '\\n', '<dedent>', 'else', ':', '\\n', '<indent>', 'chunk', '=', 'image', '.', 'getChunk', '(', '\"str\"', ')', '\\n', 'if', 'chunk', '==', 'None', ':', '\\n', '<indent>', 'return', '12', '\\n', '<dedent>', 'gammav', '=', 'unpack', '(', '\"str\"', ',', 'chunk', '.', 'content', ')', '\\n', 'if', 'gammav', '[', '0', ']', '==', '400000', ':', '\\n', '<indent>', 'return', '13', '\\n', '<dedent>', 'return', '14', '\\n', '<dedent>', '<dedent>', '<endf>']\n",
      "~~~~ COMMENTS ~~~~\n",
      "[[-1, '# Fingerprint depending on how the decoder treated the gamma information from the input image']]\n",
      "~~~~ PAIRS ~~~~~\n",
      "Pair #0\n",
      "['image', 'image', 11, [4, 50]]\n",
      "Pair #1\n",
      "['image', 'pixel', 11, [4, 50]]\n",
      "Pair #2\n",
      "['chunk', 'chunk', 58, [48, 74]]\n",
      "Pair #3\n",
      "['chunk', 'image', 58, [48, 74]]\n",
      "Pair #4\n",
      "['pixel', 'pixel', 31, [9, 21, 26]]\n",
      "Pair #5\n",
      "['pixel', 'chunk', 31, [9, 21, 26]]\n",
      "Pair #6\n",
      "['image', 'image', 50, [4, 11]]\n",
      "Pair #7\n",
      "['image', 'gammav', 50, [4, 11]]\n",
      "Pair #8\n",
      "['chunk', 'chunk', 74, [48, 58]]\n",
      "Pair #9\n",
      "['chunk', 'gammav', 74, [48, 58]]\n",
      "Pair #10\n",
      "['gammav', 'gammav', 80, [68]]\n",
      "Pair #11\n",
      "['gammav', 'image', 80, [68]]\n",
      "Pair #12\n",
      "['pixel', 'pixel', 21, [9, 26, 31]]\n",
      "Pair #13\n",
      "['pixel', 'chunk', 21, [9, 26, 31]]\n",
      "Pair #14\n",
      "['pixel', 'pixel', 26, [9, 21, 31]]\n",
      "Pair #15\n",
      "['pixel', 'gammav', 26, [9, 21, 31]]\n",
      "~~~~ TOKENS ~~~~\n",
      "['<nl>', 'def', 'physChunk', '(', 'image', ')', ':', '\\n', '<indent>', 'chunk', '=', 'image', '.', 'getChunk', '(', '\"str\"', ')', '\\n', 'if', 'chunk', '==', 'None', ':', '\\n', '<indent>', 'return', '11', '\\n', '<dedent>', 'x', ',', 'y', ',', 'u', '=', 'unpack', '(', '\"str\"', ',', 'chunk', '.', 'content', ')', '\\n', 'if', 'x', '==', '1', ':', '\\n', '<indent>', 'return', '12', '\\n', '<dedent>', 'if', 'x', '==', '1500', ':', '\\n', '<indent>', 'return', '13', '\\n', '<dedent>', 'if', 'x', '==', '1499', ':', '\\n', '<indent>', 'return', '14', '\\n', '<dedent>', 'return', '15', '\\n', '<dedent>', '<endf>']\n",
      "~~~~ COMMENTS ~~~~\n",
      "[[-1, '# Fingerprint depending on how the decoder treated the phys information in the input image'], [74, '# .net ']]\n",
      "~~~~ PAIRS ~~~~~\n",
      "Pair #0\n",
      "['chunk', 'chunk', 19, [9, 39]]\n",
      "Pair #1\n",
      "['chunk', 'x', 19, [9, 39]]\n",
      "Pair #2\n",
      "['x', 'x', 45, [29, 56, 67]]\n",
      "Pair #3\n",
      "['x', 'chunk', 45, [29, 56, 67]]\n",
      "Pair #4\n",
      "['x', 'x', 56, [29, 45, 67]]\n",
      "Pair #5\n",
      "['x', 'u', 56, [29, 45, 67]]\n",
      "Pair #6\n",
      "['x', 'x', 67, [29, 45, 56]]\n",
      "Pair #7\n",
      "['x', 'y', 67, [29, 45, 56]]\n",
      "Pair #8\n",
      "['image', 'image', 11, [4]]\n",
      "Pair #9\n",
      "['image', 'chunk', 11, [4]]\n",
      "Pair #10\n",
      "['chunk', 'chunk', 39, [9, 19]]\n",
      "Pair #11\n",
      "['chunk', 'x', 39, [9, 19]]\n",
      "~~~~ TOKENS ~~~~\n",
      "['<nl>', 'def', 'doTests', '(', 'tests', ',', 'fingerprints', ',', 'warn', ')', ':', '\\n', '<indent>', 'results', '=', '{', '}', '\\n', 'fingerprintScores', '=', '{', '}', '\\n', '<nl>', 'for', 'fingerprint', 'in', 'fingerprints', ':', '\\n', '<indent>', 'fingerprintScores', '[', 'fingerprint', '.', 'name', ']', '=', '0', '\\n', '<nl>', '<dedent>', 'for', 'test', 'in', 'tests', ':', '\\n', '<indent>', 'image', '=', 'Png', '(', 'directory', '+', 'test', '.', 'filename', '+', '\"str\"', ')', '\\n', 'if', 'not', 'image', '.', 'valid', '==', '0', ':', '\\n', '<nl>', '<indent>', 'result', '=', 'test', '.', 'function', '(', 'image', ')', '\\n', '<dedent>', 'else', ':', '\\n', '<indent>', 'result', '=', '0', '\\n', '<nl>', '<dedent>', 'results', '[', 'test', '.', 'name', ']', '=', 'result', '\\n', '<nl>', 'for', 'fingerprint', 'in', 'fingerprints', ':', '\\n', '<indent>', 'if', 'not', 'test', '.', 'name', 'in', 'fingerprint', '.', 'results', ':', '\\n', '<nl>', '<indent>', 'if', 'warn', ':', '\\n', '<indent>', 'print', '(', '\"str\"', ',', 'test', '.', 'name', ',', '\"str\"', ',', 'fingerprint', '.', 'name', ')', '\\n', '<dedent>', '<dedent>', 'elif', 'fingerprint', '.', 'results', '[', 'test', '.', 'name', ']', '==', 'result', ':', '\\n', '<indent>', 'fingerprintScores', '[', 'fingerprint', '.', 'name', ']', '+=', '1', '\\n', '<dedent>', '<dedent>', '<dedent>', 'return', 'results', ',', 'fingerprintScores', '\\n', '<dedent>', '<endf>']\n",
      "~~~~ COMMENTS ~~~~\n",
      "[[-1, \"# Test all the images in a directory (don't print warnings when generating fingerprints)\"], [22, '# Initialite the count of matching tests to zero for each fingerprint'], [39, '# Execute each test'], [70, '# Only execute the test if there is an image to test'], [90, '# Save the result of the test'], [102, '# Check if the result matches some of the fingeprints and if so, increment the match counter'], [120, '# warn if a fingerprint is missing the result for the test being run']]\n",
      "~~~~ PAIRS ~~~~~\n",
      "Pair #0\n",
      "['fingerprints', 'fingerprints', 27, [6, 106]]\n",
      "Pair #1\n",
      "['fingerprints', 'test', 27, [6, 106]]\n",
      "Pair #2\n",
      "['tests', 'tests', 45, [4]]\n",
      "Pair #3\n",
      "['tests', 'fingerprints', 45, [4]]\n",
      "Pair #4\n",
      "['result', 'result', 100, [73, 87, 155]]\n",
      "Pair #5\n",
      "['result', 'fingerprintScores', 100, [73, 87, 155]]\n",
      "Pair #6\n",
      "['fingerprints', 'fingerprints', 106, [6, 27]]\n",
      "Pair #7\n",
      "['fingerprints', 'test', 106, [6, 27]]\n",
      "Pair #8\n",
      "['results', 'results', 172, [13, 93, 118, 148]]\n",
      "Pair #9\n",
      "['results', 'result', 172, [13, 93, 118, 148]]\n",
      "Pair #10\n",
      "['fingerprintScores', 'fingerprintScores', 174, [18, 31, 159]]\n",
      "Pair #11\n",
      "['fingerprintScores', 'tests', 174, [18, 31, 159]]\n",
      "Pair #12\n",
      "['fingerprintScores', 'fingerprintScores', 31, [18, 159, 174]]\n",
      "Pair #13\n",
      "['fingerprintScores', 'fingerprint', 31, [18, 159, 174]]\n",
      "Pair #14\n",
      "['results', 'results', 93, [13, 118, 148, 172]]\n",
      "Pair #15\n",
      "['results', 'test', 93, [13, 118, 148, 172]]\n",
      "Pair #16\n",
      "['image', 'image', 79, [49, 64]]\n",
      "Pair #17\n",
      "['image', 'results', 79, [49, 64]]\n",
      "Pair #18\n",
      "['warn', 'warn', 124, [8]]\n",
      "Pair #19\n",
      "['warn', 'result', 124, [8]]\n",
      "Pair #20\n",
      "['fingerprint', 'fingerprint', 33, [25, 104, 116, 138, 146, 161]]\n",
      "Pair #21\n",
      "['fingerprint', 'image', 33, [25, 104, 116, 138, 146, 161]]\n",
      "Pair #22\n",
      "['image', 'image', 64, [49, 79]]\n",
      "Pair #23\n",
      "['image', 'test', 64, [49, 79]]\n",
      "Pair #24\n",
      "['test', 'test', 75, [43, 55, 95, 112, 132, 150]]\n",
      "Pair #25\n",
      "['test', 'results', 75, [43, 55, 95, 112, 132, 150]]\n",
      "Pair #26\n",
      "['test', 'test', 95, [43, 55, 75, 112, 132, 150]]\n",
      "Pair #27\n",
      "['test', 'results', 95, [43, 55, 75, 112, 132, 150]]\n",
      "Pair #28\n",
      "['result', 'result', 155, [73, 87, 100]]\n",
      "Pair #29\n",
      "['result', 'results', 155, [73, 87, 100]]\n",
      "Pair #30\n",
      "['test', 'test', 55, [43, 75, 95, 112, 132, 150]]\n",
      "Pair #31\n",
      "['test', 'results', 55, [43, 75, 95, 112, 132, 150]]\n",
      "Pair #32\n",
      "['test', 'test', 112, [43, 55, 75, 95, 132, 150]]\n",
      "Pair #33\n",
      "['test', 'fingerprint', 112, [43, 55, 75, 95, 132, 150]]\n",
      "Pair #34\n",
      "['fingerprint', 'fingerprint', 116, [25, 33, 104, 138, 146, 161]]\n",
      "Pair #35\n",
      "['fingerprint', 'tests', 116, [25, 33, 104, 138, 146, 161]]\n",
      "Pair #36\n",
      "['fingerprintScores', 'fingerprintScores', 159, [18, 31, 174]]\n",
      "Pair #37\n",
      "['fingerprintScores', 'tests', 159, [18, 31, 174]]\n",
      "Pair #38\n",
      "['test', 'test', 132, [43, 55, 75, 95, 112, 150]]\n",
      "Pair #39\n",
      "['test', 'result', 132, [43, 55, 75, 95, 112, 150]]\n",
      "Pair #40\n",
      "['fingerprint', 'fingerprint', 138, [25, 33, 104, 116, 146, 161]]\n",
      "Pair #41\n",
      "['fingerprint', 'warn', 138, [25, 33, 104, 116, 146, 161]]\n",
      "Pair #42\n",
      "['fingerprint', 'fingerprint', 146, [25, 33, 104, 116, 138, 161]]\n",
      "Pair #43\n",
      "['fingerprint', 'warn', 146, [25, 33, 104, 116, 138, 161]]\n",
      "Pair #44\n",
      "['test', 'test', 150, [43, 55, 75, 95, 112, 132]]\n",
      "Pair #45\n",
      "['test', 'warn', 150, [43, 55, 75, 95, 112, 132]]\n",
      "Pair #46\n",
      "['fingerprint', 'fingerprint', 161, [25, 33, 104, 116, 138, 146]]\n",
      "Pair #47\n",
      "['fingerprint', 'tests', 161, [25, 33, 104, 116, 138, 146]]\n",
      "~~~~ TOKENS ~~~~\n",
      "['<nl>', 'def', 'generateCsv', '(', 'tests', ',', 'fingerprints', ')', ':', '\\n', '<indent>', 'header', '=', '\"str\"', '\\n', 'for', 'test', 'in', 'tests', ':', '\\n', '<indent>', 'header', '=', 'header', '+', '\"str\"', '+', 'test', '.', 'name', '\\n', '<dedent>', 'print', '(', 'header', ')', '\\n', '<nl>', 'for', 'fingerprint', 'in', 'fingerprints', ':', '\\n', '<indent>', 'row', '=', 'fingerprint', '.', 'name', '\\n', 'for', 'test', 'in', 'tests', ':', '\\n', '<indent>', 'if', 'not', 'test', '.', 'name', 'in', 'fingerprint', '.', 'results', ':', '\\n', '<indent>', 'row', '+=', '\"str\"', '\\n', '<dedent>', 'else', ':', '\\n', '<indent>', 'row', '+=', '\"str\"', '+', 'str', '(', 'fingerprint', '.', 'results', '[', 'test', '.', 'name', ']', ')', '\\n', '<dedent>', '<dedent>', 'print', '(', 'row', ')', '\\n', '<dedent>', '<dedent>', '<endf>']\n",
      "~~~~ COMMENTS ~~~~\n",
      "[[-1, '# Generate a csv table with all the test results for each fingerprint (which you can then import in LibreOffice or whatever)']]\n",
      "~~~~ PAIRS ~~~~~\n",
      "Pair #0\n",
      "['tests', 'tests', 18, [4, 55]]\n",
      "Pair #1\n",
      "['tests', 'fingerprint', 18, [4, 55]]\n",
      "Pair #2\n",
      "['fingerprints', 'fingerprints', 42, [6]]\n",
      "Pair #3\n",
      "['fingerprints', 'fingerprint', 42, [6]]\n",
      "Pair #4\n",
      "['header', 'header', 35, [11, 22, 24]]\n",
      "Pair #5\n",
      "['header', 'row', 35, [11, 22, 24]]\n",
      "Pair #6\n",
      "['tests', 'tests', 55, [4, 18]]\n",
      "Pair #7\n",
      "['tests', 'header', 55, [4, 18]]\n",
      "Pair #8\n",
      "['fingerprint', 'fingerprint', 48, [40, 65, 86]]\n",
      "Pair #9\n",
      "['fingerprint', 'tests', 48, [40, 65, 86]]\n",
      "Pair #10\n",
      "['row', 'row', 100, [46, 71, 80]]\n",
      "Pair #11\n",
      "['row', 'test', 100, [46, 71, 80]]\n",
      "Pair #12\n",
      "['header', 'header', 24, [11, 22, 35]]\n",
      "Pair #13\n",
      "['header', 'fingerprints', 24, [11, 22, 35]]\n",
      "Pair #14\n",
      "['test', 'test', 28, [16, 53, 61, 90]]\n",
      "Pair #15\n",
      "['test', 'row', 28, [16, 53, 61, 90]]\n",
      "Pair #16\n",
      "['test', 'test', 61, [16, 28, 53, 90]]\n",
      "Pair #17\n",
      "['test', 'row', 61, [16, 28, 53, 90]]\n",
      "Pair #18\n",
      "['fingerprint', 'fingerprint', 65, [40, 48, 86]]\n",
      "Pair #19\n",
      "['fingerprint', 'test', 65, [40, 48, 86]]\n",
      "Pair #20\n",
      "['fingerprint', 'fingerprint', 86, [40, 48, 65]]\n",
      "Pair #21\n",
      "['fingerprint', 'tests', 86, [40, 48, 65]]\n",
      "Pair #22\n",
      "['test', 'test', 90, [16, 28, 53, 61]]\n",
      "Pair #23\n",
      "['test', 'fingerprints', 90, [16, 28, 53, 61]]\n",
      "~~~~ TOKENS ~~~~\n",
      "['<nl>', 'def', 'showResults', '(', 'scores', ',', 'nb', ')', ':', '\\n', '<indent>', 'ordered', '=', 'sorted', '(', 'iter', '(', 'scores', '.', 'items', '(', ')', ')', ',', 'key', '=', 'itemgetter', '(', '1', ')', ')', '\\n', 'for', 'result', 'in', 'ordered', ':', '\\n', '<indent>', 'print', '(', '\"str\"', '.', 'format', '(', 'result', '[', '0', ']', ',', 'result', '[', '1', ']', ',', 'nb', ')', ')', '\\n', '<dedent>', '<dedent>', '<endf>']\n",
      "~~~~ COMMENTS ~~~~\n",
      "[[-1, '# Show the fingerprinting result with the most likely library match at the bottom']]\n",
      "~~~~ PAIRS ~~~~~\n",
      "Pair #0\n",
      "['ordered', 'ordered', 35, [11]]\n",
      "Pair #1\n",
      "['ordered', 'result', 35, [11]]\n",
      "Pair #2\n",
      "['nb', 'nb', 55, [6]]\n",
      "Pair #3\n",
      "['nb', 'ordered', 55, [6]]\n",
      "Pair #4\n",
      "['scores', 'scores', 17, [4]]\n",
      "Pair #5\n",
      "['scores', 'result', 17, [4]]\n",
      "Pair #6\n",
      "['result', 'result', 45, [33, 50]]\n",
      "Pair #7\n",
      "['result', 'scores', 45, [33, 50]]\n",
      "Pair #8\n",
      "['result', 'result', 50, [33, 45]]\n",
      "Pair #9\n",
      "['result', 'nb', 50, [33, 45]]\n"
     ]
    }
   ],
   "source": [
    "for tokens, comments, scope, pairs in data:\n",
    "    print(\"~~~~ TOKENS ~~~~\")\n",
    "    print(tokens)\n",
    "    print(\"~~~~ COMMENTS ~~~~\")\n",
    "    print(comments)\n",
    "    print(\"~~~~ PAIRS ~~~~~\")\n",
    "    for i, pair in enumerate(pairs):\n",
    "        print(f\"Pair #{i}\")\n",
    "        print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ImportFrom' object has no attribute 'body'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-c0accb3b2d85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst_token\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartpos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0matok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_token\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendpos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mst1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mspecial_word\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mst1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecial_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ImportFrom' object has no attribute 'body'"
     ]
    }
   ],
   "source": [
    "a, b = atok.tree.body[0].body[0].value.body.func.first_token.startpos,\\\n",
    "atok.tree.body[0].body[0].value.body.func.last_token.endpos\n",
    "st1 = st[:a] + special_word + st[b:]\n",
    "get_tokens(st1, special_word, st[a:b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(lines_ast).intersection(set(cp.comments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lineno, line in enumerate(code.split(\"\\n\")):\n",
    "    print(lineno, line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = code.split(\"\\n\")[220][2:]\n",
    "for node in ast.walk(ast.parse(line)):\n",
    "    print(node)\n",
    "    if hasattr(node, \"lineno\"):\n",
    "        print(node.lineno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code.split(\"\\n\")[46]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fun in ast.iter_child_nodes(astree):\n",
    "    if isinstance(fun, ast.FunctionDef):\n",
    "        for node in ast.walk(fun):\n",
    "            if hasattr(node, \"comment\"):\n",
    "                print(node, node.comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node.lineno, node.col_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tokens, scope, pairs in data:\n",
    "    assert len(pairs)> 1\n",
    "    for pair in pairs:\n",
    "        iden1, iden2, loc_idx, rep_idxs = pair\n",
    "        assert len(scope) >= 3\n",
    "        assert iden1 in scope\n",
    "        assert iden2 in scope\n",
    "        assert tokens[loc_idx] == iden1\n",
    "        for rep_idx in rep_idxs:\n",
    "            assert tokens[rep_idx] == iden1\n",
    "        assert len(rep_idxs) >= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сделать проверки\n",
    "# Что еще нужно запоминать? комментарии, типы переменных? вроде я решила, что типы переменных не нужны"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from horast import parse, unparse\n",
    "import typed_ast\n",
    "\n",
    "special_word = \"JKMCFHNBVCXSDJ\"\n",
    "data = []\n",
    "\n",
    "tree = parse(read_file_to_string(filenm))\n",
    "\n",
    "for fun in typed_ast.ast3.iter_child_nodes(tree):\n",
    "    if isinstance(fun, typed_ast.ast3.FunctionDef):\n",
    "        scope = [arg.arg for arg in fun.args.args]\n",
    "        for node in typed_ast.ast3.walk(fun):\n",
    "            print(node, isinstance(node, typed_ast.ast3.Name), \\\n",
    "                  isinstance(node.ctx, typed_ast.ast3.Store) if isinstance(node, typed_ast.ast3.Name) else \"\")\n",
    "            if isinstance(node, typed_ast.ast3.Name) and isinstance(node.ctx, typed_ast.ast3.Store):\n",
    "                scope.append(node.id)\n",
    "        scope = set(scope)\n",
    "        #print(fun.name, scope)\n",
    "        #walk_ = 0\n",
    "        first = True\n",
    "        if len(scope) >= 3:\n",
    "            for node in ast.walk(fun):\n",
    "                if isinstance(node, typed_ast.ast3.Name) and isinstance(node.ctx, typed_ast.ast3.Load)\\\n",
    "                and node.id in scope:\n",
    "                    #if len(scope) >= 3 or (len(scope) >= 2 and not node.id in scope):\n",
    "                        id_ = node.id\n",
    "                        node.id = special_word\n",
    "                        tokens, idx, remember_idxs = get_tokens(unparse(fun), special_word, id_)\n",
    "                        assert tokens[idx] == special_word\n",
    "                        assert np.all([tokens[remem_idx]==id_ for remem_idx in remember_idxs])\n",
    "                        print(tokens, idx, remember_idxs)\n",
    "                        node.id = id_\n",
    "                        #if len(remember_idxs) >= 1:\n",
    "                        if first:\n",
    "                            tokens[idx] = id_\n",
    "                            data.append([tokens, scope, []])\n",
    "                            first = False\n",
    "                        data[-1][2].append([id_, id_, idx, remember_idxs]) # correct, correct\n",
    "                        buggy_id = list(scope.difference({id_}))\\\n",
    "                                   [np.random.randint(low=0, high=len(scope)-1)]\n",
    "                        data[-1][2].append([id_, buggy_id, idx, remember_idxs]) # correct, buggy\n",
    "        if not first and len(data[-1][2]) == 2:\n",
    "            del data[-1]\n",
    "                            \n",
    "                        #walk_ += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?typed_ast._ast3.arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_code = read_file_to_string(filenm)\n",
    "sys.path.append(\"py-ast-utils/\")\n",
    "import source_match\n",
    "source_match.GetSource(astree, some_code)\n",
    "assert some_code == module_node.matcher.GetSource()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'horast'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-003d2ddec8bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhorast\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\"a = 1  # a equals one after this\"\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# this will print the code with original comment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'horast'"
     ]
    }
   ],
   "source": [
    "from horast import parse, unparse\n",
    "\n",
    "tree = parse(\"\"\"a = 1  # a equals one after this\"\"\")\n",
    "print(unparse(tree))\n",
    "# this will print the code with original comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = parse(read_file_to_string(filenm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typed_ast\n",
    "typed_ast._ast3.FunctionDef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funs = []\n",
    "for node in tree.body:\n",
    "    if isinstance(node, typed_ast._ast3.FunctionDef):\n",
    "        funs.append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node.args.args[0].arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = node.body[0]\n",
    "ret.value.ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in typed_ast.ast3.walk(tree):\n",
    "    if isinstance(node, typed_ast.ast3.Name):\n",
    "        print(node.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unparse(funs[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = unparse(funs[6])\n",
    "for token in tokenize.tokenize(BytesIO(s.encode('utf-8')).readline):\n",
    "    print(token.string, token.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = set([1, 2, 3])\n",
    "s.difference({1}), s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "astor.to_source(fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_tokens(astor.to_source(fun), special_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for child in ast.iter_child_nodes(astree):\n",
    "    if isinstance(child, ast.FunctionDef) and child.name==\"gamma\":\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "child.name, child.body, child.args, list(ast.iter_child_nodes(child))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if_ = child.body[1]\n",
    "if_.body, if_.orelse, if_.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg = child.args.args[0]\n",
    "arg.arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node.id, node.col_offset, node.ctx, node.lineno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ast.Name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ast.Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenize import tokenize, untokenize, NUMBER, STRING, NAME, OP\n",
    "from io import BytesIO\n",
    "\n",
    "def decistmt(s):\n",
    "    \"\"\"Substitute Decimals for floats in a string of statements.\n",
    "\n",
    "    >>> from decimal import Decimal\n",
    "    >>> s = 'print(+21.3e-5*-.1234/81.7)'\n",
    "    >>> decistmt(s)\n",
    "    \"print (+Decimal ('21.3e-5')*-Decimal ('.1234')/Decimal ('81.7'))\"\n",
    "\n",
    "    The format of the exponent is inherited from the platform C library.\n",
    "    Known cases are \"e-007\" (Windows) and \"e-07\" (not Windows).  Since\n",
    "    we're only showing 12 digits, and the 13th isn't close to 5, the\n",
    "    rest of the output should be platform-independent.\n",
    "\n",
    "    >>> exec(s)  #doctest: +ELLIPSIS\n",
    "    -3.21716034272e-0...7\n",
    "\n",
    "    Output from calculations with Decimal should be identical across all\n",
    "    platforms.\n",
    "\n",
    "    >>> exec(decistmt(s))\n",
    "    -3.217160342717258261933904529E-7\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    g = tokenize(BytesIO(s.encode('utf-8')).readline)  # tokenize the string\n",
    "    for toknum, tokval, _, _, _ in g:\n",
    "        if toknum == NUMBER and '.' in tokval:  # replace NUMBER tokens\n",
    "            result.extend([\n",
    "                (NAME, 'Decimal'),\n",
    "                (OP, '('),\n",
    "                (STRING, repr(tokval)),\n",
    "                (OP, ')')\n",
    "            ])\n",
    "        else:\n",
    "            result.append((toknum, tokval))\n",
    "    return untokenize(result)#.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decistmt(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def 1\n",
      "greet 1\n",
      "( 53\n",
      "a 1\n",
      ") 53\n",
      ": 53\n",
      "# comment 4 55\n",
      "\n",
      " 4\n",
      "   5\n",
      "\"\"\"\n",
      "  doc\n",
      "  \"\"\" 3\n",
      "\n",
      " 4\n",
      "# comment 2 55\n",
      "\n",
      " 56\n",
      "say 1\n",
      "( 53\n",
      "\"hello\" 3\n",
      ") 53\n",
      "if 1\n",
      "a 1\n",
      "else 1\n",
      "say 1\n",
      "( 53\n",
      "\"bye\" 3\n",
      ") 53\n",
      "# comment 1 55\n",
      "\n",
      " 4\n",
      "# comment 3 55\n",
      "\n",
      " 56\n",
      "\"\"\"\n",
      "  big_coment 1  \n",
      "  \"\"\" 3\n",
      "\n",
      " 4\n",
      "return 1\n",
      "1 2\n",
      "<_ast.FunctionDef object at 0x7f04f2a53f90> ['def', 'greet', '(', 'a', ')', ':', '# comment 4', '\\n', '  ', '\"\"\"\\n  doc\\n  \"\"\"', '\\n', '# comment 2', '\\n', 'say', '(', '\"hello\"', ')', 'if', 'a', 'else', 'say', '(', '\"bye\"', ')', '# comment 1', '\\n', '# comment 3', '\\n', '\"\"\"\\n  big_coment 1  \\n  \"\"\"', '\\n', 'return', '1']\n",
      "\"\"\"\n",
      "  doc\n",
      "  \"\"\" 3\n",
      "<_ast.Expr object at 0x7f04f2a55090> ['\"\"\"\\n  doc\\n  \"\"\"']\n",
      "say 1\n",
      "( 53\n",
      "\"hello\" 3\n",
      ") 53\n",
      "if 1\n",
      "a 1\n",
      "else 1\n",
      "say 1\n",
      "( 53\n",
      "\"bye\" 3\n",
      ") 53\n",
      "<_ast.Expr object at 0x7f04f2a55110> ['say', '(', '\"hello\"', ')', 'if', 'a', 'else', 'say', '(', '\"bye\"', ')']\n",
      "\"\"\"\n",
      "  big_coment 1  \n",
      "  \"\"\" 3\n",
      "<_ast.Expr object at 0x7f04f2a55350> ['\"\"\"\\n  big_coment 1  \\n  \"\"\"']\n",
      "return 1\n",
      "1 2\n",
      "<_ast.Return object at 0x7f04f2a553d0> ['return', '1']\n",
      "a 1\n",
      "<_ast.arg object at 0x7f04f2a55050> ['a']\n",
      "\"\"\"\n",
      "  doc\n",
      "  \"\"\" 3\n",
      "<_ast.Str object at 0x7f04f2a550d0> ['\"\"\"\\n  doc\\n  \"\"\"']\n",
      "say 1\n",
      "( 53\n",
      "\"hello\" 3\n",
      ") 53\n",
      "if 1\n",
      "a 1\n",
      "else 1\n",
      "say 1\n",
      "( 53\n",
      "\"bye\" 3\n",
      ") 53\n",
      "<_ast.IfExp object at 0x7f04f2a55150> ['say', '(', '\"hello\"', ')', 'if', 'a', 'else', 'say', '(', '\"bye\"', ')']\n",
      "\"\"\"\n",
      "  big_coment 1  \n",
      "  \"\"\" 3\n",
      "<_ast.Str object at 0x7f04f2a55390> ['\"\"\"\\n  big_coment 1  \\n  \"\"\"']\n",
      "1 2\n",
      "<_ast.Num object at 0x7f04f2a55410> ['1']\n",
      "a 1\n",
      "<_ast.Name object at 0x7f04f2a55190> ['a']\n",
      "say 1\n",
      "( 53\n",
      "\"hello\" 3\n",
      ") 53\n",
      "<_ast.Call object at 0x7f04f2a551d0> ['say', '(', '\"hello\"', ')']\n",
      "say 1\n",
      "( 53\n",
      "\"bye\" 3\n",
      ") 53\n",
      "<_ast.Call object at 0x7f04f2a55290> ['say', '(', '\"bye\"', ')']\n",
      "say 1\n",
      "<_ast.Name object at 0x7f04f2a55210> ['say']\n",
      "\"hello\" 3\n",
      "<_ast.Str object at 0x7f04f2a55250> ['\"hello\"']\n",
      "say 1\n",
      "<_ast.Name object at 0x7f04f2a552d0> ['say']\n",
      "\"bye\" 3\n",
      "<_ast.Str object at 0x7f04f2a55310> ['\"bye\"']\n"
     ]
    }
   ],
   "source": [
    "import ast, asttokens\n",
    "st='''\n",
    "def greet(a): # comment 4\n",
    "  \"\"\"\n",
    "  doc\n",
    "  \"\"\"\n",
    "  # comment 2\n",
    "  say(\"hello\") if a else   say(\"bye\") # comment 1\n",
    "  # comment 3\n",
    "  \"\"\"\n",
    "  big_coment 1  \n",
    "  \"\"\"\n",
    "  return 1\n",
    "'''\n",
    "atok = asttokens.ASTTokens(st, parse=True)\n",
    "for node in ast.walk(atok.tree):\n",
    "    if hasattr(node, 'lineno'):\n",
    "        code_list = []\n",
    "        for token in atok.get_tokens(node, include_extra=True):\n",
    "            code_list.append(token.string)\n",
    "            print(token.string, token.type)\n",
    "        print(node, code_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import custom_tokenize\n",
    "import importlib\n",
    "importlib.reload(custom_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for token in tokenize.tokenize(BytesIO(read_file_to_string('new_files/CoAxLab/DeBaCl/debacl/level_set_tree.py').encode('utf-8')).readline):\n",
    "    #print(token.string, token.type)\n",
    "    tokens.append([token.string, token.type])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### TOKENIZE TEST\n",
    "st='''\n",
    "# Fingerprint depending on how the decoder treated the gamma information from the input image\n",
    "def gamma(image):\n",
    "  pixel = JKMCFHNBVCXSDJ.getPixelRgb(120,140)\n",
    "  if pixel[0]+pixel[1] + pixel[2] < 96 :\n",
    "    return 11\n",
    "  else:\n",
    "    chunk = image.getChunk(\"gAMA\")\n",
    "    if chunk == None:\n",
    "      return 12\n",
    "    gammav = unpack(\"!I\",chunk.content)\n",
    "    if gammav[0] == 400000 :\n",
    "      return 13\n",
    "    return 14\n",
    "'''\n",
    "tokens = []\n",
    "for token in tokenize.tokenize(BytesIO(st.encode('utf-8')).readline):\n",
    "    print(token.string, token.type)\n",
    "    tokens.append([token.string, token.type])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize.DEDENT, tokenize.INDENT, tokenize.COMMENT, \\\n",
    "tokenize.ENDMARKER, tokenize.ERRORTOKEN, tokenize.NEWLINE, tokenize.NL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atok.tree.body[0].body[0].value.body.func.id = \"myfun\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atok.tree.body[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = atok.tree.body[0].first_token.startpos, atok.tree.body[0].last_token.endpos\n",
    "st[a:b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in ast.walk(atok.tree):\n",
    "    if hasattr(node, 'lineno'):\n",
    "        code_list = []\n",
    "        if hasattr(node, \"id\"):\n",
    "            print(node.id)\n",
    "        for token in atok.get_tokens(node, include_extra=True):\n",
    "            code_list.append(token.string)\n",
    "        print(node, code_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?asttokens.ASTTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tokenize\n",
    "from io import BytesIO\n",
    "\n",
    "def get_tokens(f, special_word, remember_word, docstring_word=\"###DCSTR###\", docstring=\"\"):\n",
    "    #code = re.sub('\\t+', ' ', f.strip())\n",
    "    #code = re.sub(' +', ' ', code)\n",
    "    code = f\n",
    "    tokens = []\n",
    "    remember_idxs = []\n",
    "    special_idx = -1\n",
    "    comments = []\n",
    "    prev_string = None\n",
    "    for token in tokenize.tokenize(BytesIO(code.encode('utf-8')).readline):\n",
    "        if not token.type in {55}:\n",
    "            # code\n",
    "            if token.type == 3:\n",
    "                string_ = '\"str\"'\n",
    "            elif token.type == 5:\n",
    "                string_ = \"<indent>\"\n",
    "            elif token.type == 6:\n",
    "                string_ = \"<dedent>\"\n",
    "            elif token.type == 0:\n",
    "                string_ = \"<endf>\"\n",
    "            elif token.type == 54:\n",
    "                string_ = \"<errtoken>\"\n",
    "            elif token.type == 56:\n",
    "                string_ = \"<nl>\"\n",
    "            elif token.type == 4: # to avoid \"\\r\\n\" and \"\"\n",
    "                string_ = \"\\n\"\n",
    "            else:\n",
    "                string_ = token.string\n",
    "            if string_ not in {\"\\n\", \"<nl>\"} or \\\n",
    "                (string_ == \"\\n\" and prev_string != \"\\n\") or\\\n",
    "                (string_ == \"<nl>\" and prev_string != \"<nl>\"):\n",
    "                # filter out what we do not remember: repeating \\n\n",
    "                if string_ == \"\":\n",
    "                    raise ValueError(\"%s\\n%s %s %s\"%(code, \\\n",
    "                                                        token.type, token.string, prev_string))\n",
    "                tokens.append(string_)\n",
    "                if string_ == special_word:\n",
    "                    special_idx = len(tokens)-2\n",
    "                if string_ == remember_word:\n",
    "                    remember_idxs.append(len(tokens)-2)\n",
    "            prev_string = string_\n",
    "        else:\n",
    "            # comment\n",
    "            if token.string == docstring_word:\n",
    "                comments.append([len(tokens)-2, \\\n",
    "                                 docstring_word+\" \"+docstring])\n",
    "            else:\n",
    "                if len(''.join(filter(str.isalpha, token.string)))>0:\n",
    "                     comments.append([len(tokens)-2, token.string])\n",
    "    return tokens[1:], special_idx, remember_idxs, comments\n",
    "\n",
    "def get_pre_post_comments(fun, code_lines):\n",
    "    fun_line_first = fun.first_token.start[0] - 1\n",
    "    fun_line_last = fun.last_token.end[0] - 1\n",
    "    if (fun_line_first >= 2 and len(code_lines[fun_line_first-1].strip()) >= 1 and\\\n",
    "        code_lines[fun_line_first-1].strip()[0] == \"#\" \\\n",
    "        and code_lines[fun_line_first-2].strip()==\"\") or\\\n",
    "        (fun_line_first == 1 and len(code_lines[fun_line_first-1].strip()) >= 1 and \\\n",
    "        code_lines[fun_line_first-1].strip()[0] == \"#\"):\n",
    "        precomment = code_lines[fun_line_first-1].strip()+\"\\n\"\n",
    "    else:\n",
    "        precomment = \"\"\n",
    "    if (fun_line_last <= len(code_lines)-3 and len(code_lines[fun_line_last+1].strip()) >= 1 and \\\n",
    "        code_lines[fun_line_last+1].strip()[0] == \"#\" \\\n",
    "        and code_lines[fun_line_last+2].strip()==\"\") or\\\n",
    "        (fun_line_last == len(code_lines)-2 and len(code_lines[fun_line_last+1].strip()) >= 1 and\\\n",
    "         code_lines[fun_line_last+1].strip()[0] == \"#\"):\n",
    "        postcomment = \"\\n\"+code_lines[fun_line_last+1].strip()\n",
    "    else:\n",
    "        postcomment = \"\"\n",
    "    return precomment, postcomment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asttokens\n",
    "\n",
    "special_word = \"JKMCFHNBVCXSDJ\"\n",
    "data = []\n",
    "\n",
    "def process_file(filenm, data):\n",
    "\n",
    "    code = read_file_to_string(filenm)\n",
    "    code_lines = code.splitlines()\n",
    "    atok = asttokens.ASTTokens(code, parse=True)\n",
    "    astree = atok.tree\n",
    "\n",
    "\n",
    "    for fun in ast.iter_child_nodes(astree):\n",
    "        if isinstance(fun, ast.FunctionDef) and len(fun.body) > 0:\n",
    "            a_fun, b_fun = fun.first_token.startpos, fun.last_token.endpos\n",
    "            precomment, postcomment = get_pre_post_comments(fun, code_lines)\n",
    "            body_start = fun.body[0].first_token.startpos\n",
    "            docstring = ast.get_docstring(fun)\n",
    "            if not docstring:\n",
    "                docstring_placeholder = \"\"\n",
    "            else:\n",
    "                docstring_placeholder = \"\\n###DCSTR###\\n\"\n",
    "            scope = [arg.arg for arg in fun.args.args]\n",
    "            for node in ast.walk(fun):\n",
    "                # collecting scope\n",
    "                if isinstance(node, ast.Name) and isinstance(node.ctx, ast.Store):\n",
    "                    scope.append(node.id)\n",
    "            scope = set(scope)\n",
    "            first = True\n",
    "            if len(scope) >= 3:\n",
    "                for node in ast.walk(fun):\n",
    "                    if isinstance(node, ast.Name) and isinstance(node.ctx, ast.Load) and node.id in scope:\n",
    "                        id_ = node.id\n",
    "                        a, b = node.first_token.startpos, node.last_token.endpos\n",
    "                        node_code = precomment + code[a_fun:body_start]+docstring_placeholder+\\\n",
    "                        code[body_start:a] + special_word + code[b:b_fun] + postcomment\n",
    "                        tokens, idx, remember_idxs, comments = get_tokens(node_code, special_word, id_,\\\n",
    "                                                                         docstring=docstring)\n",
    "                        assert tokens[idx] == special_word\n",
    "                        assert np.all([tokens[remem_idx]==id_ for remem_idx in remember_idxs])\n",
    "                        #print(tokens, idx, remember_idxs)\n",
    "                        node.id = id_\n",
    "                        if first:\n",
    "                            tokens[idx] = id_\n",
    "                            data.append([code[a_fun:b_fun], tokens, comments, list(scope), []])\n",
    "                            first = False\n",
    "                        #data[-1][3].append([id_, id_, idx, remember_idxs]) # correct, correct\n",
    "                        buggy_id = list(scope.difference({id_}))\\\n",
    "                                   [np.random.randint(low=0, high=len(scope)-1)]\n",
    "                        data[-1][-1].append([id_, buggy_id, idx, remember_idxs]) # correct, buggy\n",
    "            if not first and len(data[-1][-1]) <= 1:\n",
    "                del data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "for r, d, f in os.walk(\"new_files/\"):\n",
    "    for file in f:\n",
    "        if file.endswith(\".py\"):\n",
    "            files.append(os.path.join(r, file))\n",
    "#process_file(filenm, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files[:100]:\n",
    "    process_file(file, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9867"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "len(json.dumps(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-43-ab93c6bc8f15>\", line 6, in <module>\n",
      "    raise ValueError(\"print\")\n",
      "ValueError: print\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "import logging\n",
    "\n",
    "try:\n",
    "    print(1)\n",
    "    raise ValueError(\"print\")\n",
    "except Exception as e:\n",
    "    print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['new_files/fingerping.py']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'new_files/CoAxLab/DeBaCl/debacl/level_set_tree.py'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-3579770c54ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_file_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"new_files/CoAxLab/DeBaCl/debacl/level_set_tree.py\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-21e924d41d7c>\u001b[0m in \u001b[0;36mread_file_to_string\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_file_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'new_files/CoAxLab/DeBaCl/debacl/level_set_tree.py'"
     ]
    }
   ],
   "source": [
    "print(read_file_to_string(\"new_files/CoAxLab/DeBaCl/debacl/level_set_tree.py\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['new_files/fingerping.py']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<nl>', 'def', 'gamma', '(', 'image', ')', ':', '\\n', '<indent>', 'pixel', '=', 'image', '.', 'getPixelRgb', '(', '120', ',', '140', ')', '\\n', 'if', 'pixel', '[', '0', ']', '+', 'pixel', '[', '1', ']', '+', 'pixel', '[', '2', ']', '<', '96', ':', '\\n', '<indent>', 'return', '11', '\\n', '<dedent>', 'else', ':', '\\n', '<indent>', 'chunk', '=', 'image', '.', 'getChunk', '(', '\"str\"', ')', '\\n', 'if', 'chunk', '==', 'None', ':', '\\n', '<indent>', 'return', '12', '\\n', '<dedent>', 'gammav', '=', 'unpack', '(', '\"str\"', ',', 'chunk', '.', 'content', ')', '\\n', 'if', 'gammav', '[', '0', ']', '==', '400000', ':', '\\n', '<indent>', 'return', '13', '\\n', '<dedent>', 'return', '14', '\\n', '<dedent>', '<dedent>', '<endf>']\n",
      "[[-1, '# Fingerprint depending on how the decoder treated the gamma information from the input image']]\n",
      "['image', 'gammav', 11, [4, 50]]\n",
      "['chunk', 'pixel', 58, [48, 74]]\n",
      "['pixel', 'image', 31, [9, 21, 26]]\n",
      "['image', 'gammav', 50, [4, 11]]\n",
      "['chunk', 'gammav', 74, [48, 58]]\n",
      "['gammav', 'chunk', 80, [68]]\n",
      "['pixel', 'gammav', 21, [9, 26, 31]]\n",
      "['pixel', 'gammav', 26, [9, 21, 31]]\n",
      "['<nl>', 'def', 'physChunk', '(', 'image', ')', ':', '\\n', '<indent>', 'chunk', '=', 'image', '.', 'getChunk', '(', '\"str\"', ')', '\\n', 'if', 'chunk', '==', 'None', ':', '\\n', '<indent>', 'return', '11', '\\n', '<dedent>', 'x', ',', 'y', ',', 'u', '=', 'unpack', '(', '\"str\"', ',', 'chunk', '.', 'content', ')', '\\n', 'if', 'x', '==', '1', ':', '\\n', '<indent>', 'return', '12', '\\n', '<dedent>', 'if', 'x', '==', '1500', ':', '\\n', '<indent>', 'return', '13', '\\n', '<dedent>', 'if', 'x', '==', '1499', ':', '\\n', '<indent>', 'return', '14', '\\n', '<dedent>', 'return', '15', '\\n', '<dedent>', '<endf>']\n",
      "[[-1, '# Fingerprint depending on how the decoder treated the phys information in the input image'], [74, '# .net ']]\n",
      "['chunk', 'u', 19, [9, 39]]\n",
      "['x', 'chunk', 45, [29, 56, 67]]\n",
      "['x', 'y', 56, [29, 45, 67]]\n",
      "['x', 'u', 67, [29, 45, 56]]\n",
      "['image', 'chunk', 11, [4]]\n",
      "['chunk', 'y', 39, [9, 19]]\n",
      "['<nl>', 'def', 'doTests', '(', 'tests', ',', 'fingerprints', ',', 'warn', ')', ':', '\\n', '<indent>', 'results', '=', '{', '}', '\\n', 'fingerprintScores', '=', '{', '}', '\\n', '<nl>', 'for', 'fingerprint', 'in', 'fingerprints', ':', '\\n', '<indent>', 'fingerprintScores', '[', 'fingerprint', '.', 'name', ']', '=', '0', '\\n', '<nl>', '<dedent>', 'for', 'test', 'in', 'tests', ':', '\\n', '<indent>', 'image', '=', 'Png', '(', 'directory', '+', 'test', '.', 'filename', '+', '\"str\"', ')', '\\n', 'if', 'not', 'image', '.', 'valid', '==', '0', ':', '\\n', '<nl>', '<indent>', 'result', '=', 'test', '.', 'function', '(', 'image', ')', '\\n', '<dedent>', 'else', ':', '\\n', '<indent>', 'result', '=', '0', '\\n', '<nl>', '<dedent>', 'results', '[', 'test', '.', 'name', ']', '=', 'result', '\\n', '<nl>', 'for', 'fingerprint', 'in', 'fingerprints', ':', '\\n', '<indent>', 'if', 'not', 'test', '.', 'name', 'in', 'fingerprint', '.', 'results', ':', '\\n', '<nl>', '<indent>', 'if', 'warn', ':', '\\n', '<indent>', 'print', '(', '\"str\"', ',', 'test', '.', 'name', ',', '\"str\"', ',', 'fingerprint', '.', 'name', ')', '\\n', '<dedent>', '<dedent>', 'elif', 'fingerprint', '.', 'results', '[', 'test', '.', 'name', ']', '==', 'result', ':', '\\n', '<indent>', 'fingerprintScores', '[', 'fingerprint', '.', 'name', ']', '+=', '1', '\\n', '<dedent>', '<dedent>', '<dedent>', 'return', 'results', ',', 'fingerprintScores', '\\n', '<dedent>', '<endf>']\n",
      "[[-1, \"# Test all the images in a directory (don't print warnings when generating fingerprints)\"], [22, '# Initialite the count of matching tests to zero for each fingerprint'], [39, '# Execute each test'], [70, '# Only execute the test if there is an image to test'], [90, '# Save the result of the test'], [102, '# Check if the result matches some of the fingeprints and if so, increment the match counter'], [120, '# warn if a fingerprint is missing the result for the test being run']]\n",
      "['fingerprints', 'fingerprint', 27, [6, 106]]\n",
      "['tests', 'test', 45, [4]]\n",
      "['result', 'fingerprint', 100, [73, 87, 155]]\n",
      "['fingerprints', 'image', 106, [6, 27]]\n",
      "['results', 'warn', 172, [13, 93, 118, 148]]\n",
      "['fingerprintScores', 'warn', 174, [18, 31, 159]]\n",
      "['fingerprintScores', 'fingerprint', 31, [18, 159, 174]]\n",
      "['results', 'tests', 93, [13, 118, 148, 172]]\n",
      "['image', 'fingerprint', 79, [49, 64]]\n",
      "['warn', 'fingerprint', 124, [8]]\n",
      "['fingerprint', 'fingerprintScores', 33, [25, 104, 116, 138, 146, 161]]\n",
      "['image', 'fingerprints', 64, [49, 79]]\n",
      "['test', 'fingerprint', 75, [43, 55, 95, 112, 132, 150]]\n",
      "['test', 'fingerprintScores', 95, [43, 55, 75, 112, 132, 150]]\n",
      "['result', 'results', 155, [73, 87, 100]]\n",
      "['test', 'fingerprints', 55, [43, 75, 95, 112, 132, 150]]\n",
      "['test', 'image', 112, [43, 55, 75, 95, 132, 150]]\n",
      "['fingerprint', 'test', 116, [25, 33, 104, 138, 146, 161]]\n",
      "['fingerprintScores', 'test', 159, [18, 31, 174]]\n",
      "['test', 'result', 132, [43, 55, 75, 95, 112, 150]]\n",
      "['fingerprint', 'test', 138, [25, 33, 104, 116, 146, 161]]\n",
      "['fingerprint', 'fingerprints', 146, [25, 33, 104, 116, 138, 161]]\n",
      "['test', 'tests', 150, [43, 55, 75, 95, 112, 132]]\n",
      "['fingerprint', 'warn', 161, [25, 33, 104, 116, 138, 146]]\n",
      "['<nl>', 'def', 'generateCsv', '(', 'tests', ',', 'fingerprints', ')', ':', '\\n', '<indent>', 'header', '=', '\"str\"', '\\n', 'for', 'test', 'in', 'tests', ':', '\\n', '<indent>', 'header', '=', 'header', '+', '\"str\"', '+', 'test', '.', 'name', '\\n', '<dedent>', 'print', '(', 'header', ')', '\\n', '<nl>', 'for', 'fingerprint', 'in', 'fingerprints', ':', '\\n', '<indent>', 'row', '=', 'fingerprint', '.', 'name', '\\n', 'for', 'test', 'in', 'tests', ':', '\\n', '<indent>', 'if', 'not', 'test', '.', 'name', 'in', 'fingerprint', '.', 'results', ':', '\\n', '<indent>', 'row', '+=', '\"str\"', '\\n', '<dedent>', 'else', ':', '\\n', '<indent>', 'row', '+=', '\"str\"', '+', 'str', '(', 'fingerprint', '.', 'results', '[', 'test', '.', 'name', ']', ')', '\\n', '<dedent>', '<dedent>', 'print', '(', 'row', ')', '\\n', '<dedent>', '<dedent>', '<endf>']\n",
      "[[-1, '# Generate a csv table with all the test results for each fingerprint (which you can then import in LibreOffice or whatever)']]\n",
      "['tests', 'test', 18, [4, 55]]\n",
      "['fingerprints', 'test', 42, [6]]\n",
      "['header', 'row', 35, [11, 22, 24]]\n",
      "['tests', 'fingerprint', 55, [4, 18]]\n",
      "['fingerprint', 'tests', 48, [40, 65, 86]]\n",
      "['row', 'fingerprint', 100, [46, 71, 80]]\n",
      "['header', 'test', 24, [11, 22, 35]]\n",
      "['test', 'tests', 28, [16, 53, 61, 90]]\n",
      "['test', 'header', 61, [16, 28, 53, 90]]\n",
      "['fingerprint', 'row', 65, [40, 48, 86]]\n",
      "['fingerprint', 'row', 86, [40, 48, 65]]\n",
      "['test', 'row', 90, [16, 28, 53, 61]]\n",
      "['<nl>', 'def', 'showResults', '(', 'scores', ',', 'nb', ')', ':', '\\n', '<indent>', 'ordered', '=', 'sorted', '(', 'iter', '(', 'scores', '.', 'items', '(', ')', ')', ',', 'key', '=', 'itemgetter', '(', '1', ')', ')', '\\n', 'for', 'result', 'in', 'ordered', ':', '\\n', '<indent>', 'print', '(', '\"str\"', '.', 'format', '(', 'result', '[', '0', ']', ',', 'result', '[', '1', ']', ',', 'nb', ')', ')', '\\n', '<dedent>', '<dedent>', '<endf>']\n",
      "[[-1, '# Show the fingerprinting result with the most likely library match at the bottom']]\n",
      "['ordered', 'scores', 35, [11]]\n",
      "['nb', 'result', 55, [6]]\n",
      "['scores', 'result', 17, [4]]\n",
      "['result', 'ordered', 45, [33, 50]]\n",
      "['result', 'nb', 50, [33, 45]]\n"
     ]
    }
   ],
   "source": [
    "for source, tokens, comments, scope, pairs in data:\n",
    "    print(tokens)\n",
    "    print(comments)\n",
    "    for pair in pairs:\n",
    "        print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for source, tokens, comments, scope, pairs in data:\n",
    "    assert len(pairs)> 1\n",
    "    for pair in pairs:\n",
    "        iden1, iden2, loc_idx, rep_idxs = pair\n",
    "        assert len(scope) >= 3\n",
    "        assert iden1 in scope\n",
    "        assert iden2 in scope\n",
    "        assert tokens[loc_idx] == iden1\n",
    "        for rep_idx in rep_idxs:\n",
    "            assert tokens[rep_idx] == iden1\n",
    "        assert len(rep_idxs) >= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for source, tokens, comments, scope, pairs in data:\n",
    "    print(\"----------------Data-----------------------\")\n",
    "    print(source)\n",
    "    print(\"---------------Ours----------------\")\n",
    "    offset = 0\n",
    "    for loc, com in comments:\n",
    "        tokens = tokens[:loc+offset+1] + [com+\"\\n\"] + tokens[loc+offset+1:]\n",
    "        offset +=1\n",
    "    print(\" \".join(tokens))\n",
    "    print(\"----------------------------------------------\")\n",
    "    print(\"----------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install eradicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eradicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for source, tokens, comments, scope, pairs in data:\n",
    "    for loc, com in comments:\n",
    "        com_filtered = \"\\n\".join(list(eradicate.filter_commented_out_code\\\n",
    "                             (com, aggressive=False)))\n",
    "        if com_filtered == \"\":\n",
    "            print(com)\n",
    "            print(com_filtered)\n",
    "            print(\"----------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "for r, d, f in os.walk(\"data\"):\n",
    "    for file in f:\n",
    "        if file.endswith(\".py\"):\n",
    "            files.append(os.path.join(r, file))\n",
    "#process_file(filenm, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files[464]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_folder = \"data_3\"\n",
    "os.makedirs(new_folder, exist_ok=True)\n",
    "for file in files:\n",
    "    try:\n",
    "        os.system('~/miniconda3/envs/python2/bin/2to3 data/0xcite/fingerping/fingerping.py -o new_files -w -n --no-diffs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_folder = \"data_3\"\n",
    "os.makedirs(new_folder, exist_ok=True)\n",
    "file = files[3]\n",
    "os.system(\"~/miniconda3/envs/python2/bin/2to3 %s -o %s -w -n --no-diffs\"%\\\n",
    "                  (file, new_folder+file[4:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "def walk(node):\n",
    "    \"\"\"\n",
    "    Recursively yield all descendant nodes in the tree starting at *node*\n",
    "    (including *node* itself), in no specified order.  This is useful if you\n",
    "    only want to modify nodes in place and don't care about the context.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    todo = deque([node])\n",
    "    while todo:\n",
    "        node = todo.popleft()\n",
    "        if isinstance(node, ast.FunctionDef):\n",
    "            count += 1\n",
    "            print(\"here\")\n",
    "        else:\n",
    "            todo.extend(ast.iter_child_nodes(node))\n",
    "            print(\"here\")\n",
    "        #yield node\n",
    "    print(\"here\", count)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk(atok.tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = np.random.randint(len(files))\n",
    "print(num)\n",
    "print(read_file_to_string(files[num]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in ast.walk(ast.parse(read_file_to_string(files[num]))):\n",
    "    if isinstance(node, ast.ClassDef):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node.body[6].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for child in ast.iter_child_nodes(node):\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scs-ext",
   "language": "python",
   "name": "scs-ext"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
